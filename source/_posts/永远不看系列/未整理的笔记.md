---
title: 未整理的笔记
date: 2023-09-26 17:45:00
categories: 永远不看系列
permalink: wait.html
---

## JSON

JSON（JavaScript Object Notation，JavaScript 对象表示法）是一种轻量级的数据交换格式，可以看成是字符串。和 JavaScript 没什么关系，就像 JavaScript 和 Java 没什么关系一样。从 Python 的角度来看，一个 JSON 文件是一系列列表和字典的嵌套。

JS：

```js
jsonStr = JSON.stringify(jsObj);
jsObj = JSON.parse(jsonStr);
```

Python：

```python
import json
json_str = json.dumps(py_obj)
py_obj = json.loads(json_str)
```

<!--more-->

## 解决 VSCode 集成终端按键与快捷键冲突

`Ctrl + Shift + P` 打开命令面板，搜键盘快捷方式，搜冲突的按键，右键更改 When 表达式，加上 `&& !terminalFocus`

## 远程附加调试 Hadoop

hadoop-env.sh

```sh
export 某某_OPTS="-agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=n"
```

launch.json

```json
{
  "version": "0.2.0",
  "configurations": [
    {
      "type": "java",
      "name": "Hadoop 某某",
      "request": "attach",
      "hostName": "ubuntu101",
      "port": 8888
    }
  ]
}
```

## 用 browser-sync 实现在文件内容改变后立即刷新页面

起因是：不管是直接打开 html 文件，还是用`python -m http.server 8000 --bind 127.0.0.1`，在修改保存 html 文件后浏览器都不能立马刷新。

```sh
npm install browser-sync
```

```sh
npx browser-sync start --server --files "."
```

`--files` 后指定要监视的文件。

`.` 表示监视当前目录以及子目录下的所有文件。

可以改成 `*.html`，`public/*.*` 等。

html 文件里至少得有一对`<body>`标签，要么没法自动刷新。

## MapReduce 编程入门

[官方教程](https://github.com/apache/hadoop/blob/branch-3.3.6/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/markdown/MapReduceTutorial.md)

<img src="/blog/images/MapReduce.webp">

我们想统计一篇英文文章里的单词，每个都出现了多少次。假设这个文本文件只有英文单词、空格和换行符（LF 或者 CRLF）。

需要继承 Mapper 类，重写它的 map() 方法。这个方法里传了三个参数：该行的偏移量、该行的文本内容、环境上下文。在这个方法里，对每一行按空格进行分割，形成一个 String[]，遍历这个 String[]，把它写到上下文里，这样写的就是一系列 `<word,1>`。

```java
public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
    // KEYIN（偏移量），VALUEIN（每一行的文本），KEYOUT（单词），VALUEOUT（1）
    // 对每一行进行分割
    private Text wordText = new Text();// 向 context 里写的 KEYOUT。Text 相当于一个盒子

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String[] words = line.split(" ");

        int i = 1;
        System.out.println(i++ + " | " + key + " | " + value + " | " + line);
        // input 有三行文本
        // 输出了三次，每次为：1 | 偏移量 | 该行内容（不带换行符） | 该行内容（不带换行符）
        // 其中 1 没有变，说明是分布执行的
        // 第一次偏移量为 0，第一行有 35 个英文字母 + 空格，UTF-8 编码，加上 CRLF，第二次偏移量是 37

        for (String word : words) {
            wordText.set(word);
            context.write(wordText, new IntWritable(1));
            System.out.println(context.getCurrentKey() + " | " + context.getCurrentValue());
            // 这行语句每次输出的 key 和 value 和当前 map() 传的参相同
        }
    }
}
```

继承 Reducer 类，重写 reduce() 方法。

```java
public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
    // KEYIN 和 VALUEIN 来自 WordCountMapper.map() 里向 context 里写的
    // 每一个 reducer 处理的是 【相同的KEYIN】 的 【VALUEIN集合】
    IntWritable outV = new IntWritable();// VALUEOUT

    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context)
            throws IOException, InterruptedException {
        int sum = 0;// 单词计数

        for (IntWritable count : values) {
            sum += count.get();
            System.out.println(count.get());// values 的每一个 count 都是 1
        }

        outV.set(sum);
        context.write(key, outV);
    }
}
```

继承 Driver 类，进行一些配置。

```java
public class WordCountDriver {
    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);
        job.setJarByClass(WordCountDriver.class);
        job.setMapperClass(WordCountMapper.class);
        job.setReducerClass(WordCountReducer.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        FileInputFormat.setInputPaths(job, new Path("input.txt"));
        FileOutputFormat.setOutputPath(job, new Path("output"));
        boolean result = job.waitForCompletion(true);
        System.exit(result ? 0 : 1);
    }
}
```

### 本地测试

[winutils](https://github.com/cdarlint/winutils)，可以先用 3.3.5 的。

### 集群测试

改 `WordCountDriver.java` 的 IO 路径为传参：

```java
FileInputFormat.setInputPaths(job, new Path(args[0]));
FileOutputFormat.setOutputPath(job, new Path(args[1]));
```

`FileAlreadyExistsException`：不要提前建好输出文件夹

## Hadoop 的序列化案例

序列化（serialization）是指将内存中的对象转换成字节序列，以便传输和持久化存储（硬盘）。

反序列化是反过来，把存储在硬盘中的持久化数据转换为内存中的对象。

例如 JSON，它的文本文件属于持久化数据，读 json 文件后转换成对象属于反序列化。

为什么要序列化：内存中的对象是鲜活的，需要把它序列化成标准形式，便于持久化管理和多台计算机通信。对象是“声音”，序列化后是“文字”。

案例：获取 `score.txt` 里各科成绩的最高分信息

### score.txt

```
张三 语文 73
张三 数学 97
李四 语文 106
...
```

### ScoreBean.java

```java
class ScoreBean implements Writable {
    String studentName;
    String courseName;
    int score;

    public ScoreBean() {
    }

    @Override
    public void write(DataOutput out) throws IOException {
        // 序列化
        // 这个和下面那个方法, 如果少写了, 会出现 null, 数字变奇怪 之类的错误
        out.writeUTF(studentName);
        out.writeUTF(courseName);
        out.writeInt(score);
    }

    @Override
    public void readFields(DataInput in) throws IOException {
        // 反序列化
        studentName = in.readUTF();
        courseName = in.readUTF();
        score = in.readInt();
    }

    @Override
    public String toString() {
        // 好像在 reducer 里的 context.write() 里传 VALUEOUT 后会调用
        // 如果不重写，会返回类似 "test.hrf.hd.ScoreBean@1aae33be"
        return studentName + "\t" + courseName + "\t" + score;
    }

    public String getStudentName() {
        return studentName;
    }

    public void setStudentName(String studentName) {
        this.studentName = studentName;
    }

    public String getCourseName() {
        return courseName;
    }

    public void setCourseName(String courseName) {
        this.courseName = courseName;
    }

    public int getScore() {
        return score;
    }

    public void setScore(int score) {
        this.score = score;
    }
}
```

### ScoreStatsMapper.java

```java
class ScoreStatsMapper extends Mapper<LongWritable, Text, Text, ScoreBean> {
    // KEYIN（偏移量），VALUEIN（每一行的文本），KEYOUT（courseName），VALUEOUT（ScoreBean）

    private Text outK = new Text();// courseName
    private ScoreBean outV = new ScoreBean();

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String[] split = line.split(" ");

        String studentName = split[0];
        String courseName = split[1];
        int score = Integer.parseInt(split[2]);

        outK.set(courseName);// 因为需要找的是对于每门课的最高成绩
        outV.setStudentName(studentName);
        outV.setCourseName(courseName);
        outV.setScore(score);

        context.write(outK, outV);
    }
}
```

### ScoreStatsReducer.java

```java
public class ScoreStatsReducer extends Reducer<Text, ScoreBean, Text, ScoreBean> {
    private ScoreBean outV = new ScoreBean();

    @Override
    protected void reduce(Text key, Iterable<ScoreBean> values, Context context)
            throws IOException, InterruptedException {
        // 这里的 key 是 mapper 的 KEYOUT, 即 courseName
        // 这里的 values 是 map 后同名 key 的 VALUEOUT 集合, 即同课程名的 ScoreBean 集合
        int currentMaxScore = 0;
        for (ScoreBean scoreBean : values) {

            System.out.println(scoreBean.toString());

            int currentScore = scoreBean.getScore();
            if (currentScore > currentMaxScore) {
                currentMaxScore = currentScore;

                outV = new ScoreBean();// 如果用浅拷贝, 会在退出循环后变成最后一个
                outV.setStudentName(scoreBean.getStudentName());
                outV.setCourseName(scoreBean.getCourseName());
                outV.setScore(currentMaxScore);
            }
        }

        context.write(key, outV);
    }
}
```

### ScoreStatsDriver.java

适当改 Path

```java
public class ScoreStatsDriver {
    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);
        job.setJarByClass(ScoreStatsDriver.class);
        job.setMapperClass(ScoreStatsMapper.class);
        job.setReducerClass(ScoreStatsReducer.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(ScoreBean.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(ScoreBean.class);
        FileInputFormat.setInputPaths(job, new Path("score.txt"));
        FileOutputFormat.setOutputPath(job, new Path("output"));
        boolean result = job.waitForCompletion(true);
        System.exit(result ? 0 : 1);
    }
}
```

### 输出结果

```
化学	王五	化学	84
数学	王五	数学	143
物理	张三	物理	72
生物	陈六	生物	78
英语	陈六	英语	68
语文	王五	语文	108
```

## 对 Hadoop 集群启动脚本的注解

r3.3.6

- [UnixShellAPI](https://hadoop.apache.org/docs/r3.3.6/hadoop-project-dist/hadoop-common/UnixShellAPI.html)

### $HADOOP_HOME/sbin/start-all.sh

对应 https://github.com/apache/hadoop/blob/branch-3.3.6/hadoop-common-project/hadoop-common/src/main/bin/start-all.sh

干的事：

1. 【28-36】找目录 `${HADOOP_HOME}/libexec`，里面有其他脚本，和配置文件 `hadoop-config.sh`
2. 【38-44】把 `hadoop-config.sh` 包含到当前脚本里
3. 【46-53】如果 `hadoop_privilege_check` 返回 0，进行 10 秒的启动前确认
4. 【55-63】运行另两个启动脚本 `start-dfs.sh` 和 `start-yarn.sh`

- 28：if [[-n]] 判断字符串非空，非空为 1
- 31：获取当前脚本的完整路径。$0 表示当前脚本的名称（含扩展名）
- 32：-P 表示使用物理路径，不使用软链接。-- 表示后面跟的不是选项了，是参数。对于 dirname 命令，如果路径不以/结束，返回最后一个/之前的内容；如果路径以/结束，返回倒数第二个/之前的内容。>/dev/null 就是把输出重定向到 /dev/null，丢弃输出的数据。pwd 获取当前目录的绝对路径
- 37：注释告诉 ShellCheck 工具忽略 SC2034 警告（定义了未使用的变量）
- 39：-f 用于检查文件是否存在，并且是一个常规文件（不是目录、符号链接、设备文件或管道）
- 40：. 将另一个脚本包含到当前脚本中
- 42：2>&1 将标准错误输出重定向到标准输出
- 47：trap 设置信号处理程序
- 52：取消对 INT 信号的捕获

### $HADOOP_HOME/sbin/start-dfs.sh

对应 https://github.com/apache/hadoop/blob/branch-3.3.6/hadoop-hdfs-project/hadoop-hdfs/src/main/bin/start-dfs.sh

干的事：

1. 【43-61】同样找 `${HADOOP_HOME}/libexec` 目录，并包含 `hdfs-config.sh`
2. 【63-78】对本脚本传参个数大于等于一个时，如果第一个参数为 `-upgrade` 或 `-rollback`，把它赋值给 `nameStartOpt` 或 `dataStartOpt`

```
- 64: $# 参数个数，-ge 大于等于
- 66：shift 把所有参数“左移”一位（丢弃第一个参数，原来第二个参数变成第一个参数……）
- 82：$\* 所有当前参数（shift 后的）拼接起来的字符串
- 89：if [[-z]] 判断字符串空，空为 1
- 101：$? 上一个进程结束的状态码，0 成功 1 失败
- 112：(( )) 可在里面进行算术运算并赋值
- 121：=~ 正则匹配
- 147：2>&- 把标准错误输出重定向到“关闭”
- 149：`"${#JOURNAL_NODES}"` 获取 JOURNAL_NODES 变量存储的字符串的长度
- 163：| tr '[:upper:]' '[:lower:]' 把前面的输出结果通过管道传递给 tr 命令，转换成小写字母
```

## ZooKeeper 的 Shell 命令和 Java API 操作

### shell 命令

前提是服务端正常（半数以上节点存活），在启动了某一台存活机器上的客户端后的操作：

见 `$ZK_HOME/docs/zookeeperCLI.html`

https://zookeeper.apache.org/doc/r3.8.2/zookeeperCLI.html

- 创建节点：`create [-s] [-e] /path/to/node "content"`
  - `-s`：带序号，就是在节点名后面加上数字
  - `-e`：临时节点，在【创建此节点的那一方客户端退出】后会被删除
- 查看节点详细信息：`get -s /path/to/node`（不 `-s` 时只查看值）（与 `stat /path/to/node` 效果一样）
- 修改节点数据值：`set /path/to/node "content"`
- 删除与递归删除节点：`delete /path/to/node` `deleteall /path/to/node`
- 节点的值变化监听： `get -w /path/to/node` （节点被删除时也会被捕获）
- 节点的子节点（路径）变化监听： `ls -w /path/to/node` （节点值变化时不会被捕获）
- 列出路径下的所有子节点：`ls -R /path`

### JavaAPI 操作

使用了 [JUnit5](https://junit.org/junit5/docs/current/user-guide/) 测试框架，在 `工程目录/src/test/java` 下创建 java 源文件进行测试。

4. 适当修改 pom.xml 里的 `<maven.compiler.source>` 和 `<maven.compiler.target>` 的 JDK 版本。
5. `mvn install`
6. `src/main/resources` 里加 `log4j.properties` 配置文件：

   ```properties
   log4j.rootLogger=INFO, stdout
   log4j.appender.stdout=org.apache.log4j.ConsoleAppender
   log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
   log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n
   log4j.appender.logfile=org.apache.log4j.FileAppender
   log4j.appender.logfile.File=target/spring.log
   log4j.appender.logfile.layout=org.apache.log4j.PatternLayout
   log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n
   ```

7. 在 `src/test/java` 下建 `zkClient.java`

### 创建 Znode

```java
public class zkClient {
    private static String connectString = "ubuntu101:2181";
    private static int sessionTimeout = 100000;
    private ZooKeeper zkClient = null;

    @BeforeEach
    public void init() throws IOException {
        zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() {
            @Override
            public void process(WatchedEvent watchedEvent) {
            }
        });
    }

    // 该方法在测试时只会被调用一次
    @Test
    public void test() throws KeeperException, InterruptedException {
        // 创建一个所有客户端都可读写、持久的节点
        zkClient.create("/fruit1", "apple".getBytes(),
                ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
        System.out.println("节点创建成功");
    }
}
```

### 获取子节点并监听节点变化

```java
public class zkClient {
    private static String connectString = "ubuntu101:2181";
    private static int sessionTimeout = 100000;
    private ZooKeeper zkClient = null;

    @BeforeEach
    public void init() throws IOException {
        zkClient = new ZooKeeper(connectString, sessionTimeout,
                new Watcher() {
                    @Override
                    public void process(WatchedEvent watchedEvent) {
                        System.out.println("---start-------------------------------------");
                        List<String> children = null;
                        try {
                            children = zkClient.getChildren("/", true);
                        } catch (KeeperException | InterruptedException e) {
                            e.printStackTrace();
                        }
                        children.forEach(System.out::println);
                        System.out.println("---end--------------------------------------");
                    }
                });
    }

    // 该方法在测试时只会被调用一次
    @Test
    public void test() throws InterruptedException {
        Thread.sleep(Long.MAX_VALUE);
    }
}
```

### 判断节点是否存在

```java
public class zkClient {
    private static String connectString = "ubuntu101:2181";
    private static int sessionTimeout = 100000;
    private ZooKeeper zkClient = null;

    @BeforeEach
    public void init() throws IOException {
        zkClient = new ZooKeeper(connectString, sessionTimeout,
                new Watcher() {
                    @Override
                    public void process(WatchedEvent watchedEvent) {
                    }
                });
    }

    // 该方法在测试时只会被调用一次
    @Test
    public void test() throws KeeperException, InterruptedException {
        Stat sta = zkClient.exists("/fruit", false);
        System.out.println(null == sta ? "不存在" : "存在");
    }
}
```

### 服务器动态上下线案例

模拟服务器动态上下线：

1. 提前创建 `/servers` 节点
2. - DistributeClient 监听服务器上下线（监听`/servers`下子路径的变化）
   - DistributeServer 模拟服务器上下线（创建/删除`/servers/hostname`）
3. 上/下线后的业务逻辑

`DistributeClient.java`：

```java
public class DistributeClient {

    private String connectString = "ubuntu101:2181,ubuntu102:2181,ubuntu103:2181";
    private int sessionTimeout = 100000;
    private ZooKeeper zk;

    public static void main(String[] args) throws IOException, KeeperException, InterruptedException {
        DistributeClient client = new DistributeClient();

        // 1 获取ZK连接
        client.getConnect();
        // 2 监听服务器下/servers下面子节点的增加和删除
        client.getServerList();

        // 3 相关业务逻辑（主要是添加延迟函数）
        client.buisiness();
    }

    private void buisiness() throws InterruptedException {
        Thread.sleep(Long.MAX_VALUE);
    }

    private void getServerList() throws KeeperException, InterruptedException {
        // 第二个参数设置为true,监听走的是getConnect下的process方法。
        List<String> children = zk.getChildren("/servers", true);
        // 循环取出每一个节点对应的主机名称
        ArrayList<Object> servers = new ArrayList<>();// 存储servers下的所有节点名称
        for (String child : children) {
            byte[] data = zk.getData("/servers/" + child, false, null);
            servers.add(new String(data));
        }
        // 打印
        System.out.println(servers);
    }

    private void getConnect() throws IOException {
        zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() {
            @Override
            public void process(WatchedEvent watchedEvent) {
                // 因为注册一次，只能监听一次，所以需要在监听方法里面再注册一下
                try {
                    getServerList();
                } catch (KeeperException e) {
                    e.printStackTrace();
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
            }
        });
    }
}
```

`DistributeServer.java`：

```java
public class DistributeServer {

    private String connectString = "ubuntu101:2181,ubuntu102:2181";
    private int sessionTimeout = 2000;
    private ZooKeeper zk;

    public static void main(String[] args) throws IOException, KeeperException, InterruptedException {

        DistributeServer server = new DistributeServer();
        // 1. 连接zookeeper集群
        server.getConnect();

        // 2. 向集群进行注册（也就是创建服务器的路径）
        server.regist(args[0]);

        // 3. 业务逻辑的代码
        // 添加延迟函数，防止程序结束了，看不到节点的相关变化
        server.business();
    }

    private void business() throws InterruptedException {
        Thread.sleep(Long.MAX_VALUE);
    }

    private void regist(String hostname) throws KeeperException, InterruptedException {
        // znode 将在客户端断开连接时被删除，并且其名称将附加单调递增的数字
        String create = zk.create("/servers/" + hostname, hostname.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE,
                CreateMode.EPHEMERAL_SEQUENTIAL);
        System.out.println(hostname + " is online");
    }

    private void getConnect() throws IOException {
        zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() {
            @Override
            public void process(WatchedEvent watchedEvent) {

            }
        });
    }
}
```

## YARN 概述及案例

Yet Another Resouce Negotiator，另一种资源调度器，为什么叫另一种：新的

<img src="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif">

- 资源管理器 ResourceManager：集群老大
- 节点管理器 NodeManager：节点老大
- 容器 Container：每个节点里的多个小电脑
- ApplicatonMaster：用户提交的应用程序里的老大

### 需求

需求：从 1G 数据中，统计每个单词出现次数。

需求分析：
1G / 128m = 8 个 MapTask；1 个 ReduceTask；1 个 mrAppMaster
平均每个节点运行 10 个 / 3 台 ≈ 3 个任务（3 3 4）

修改 `yarn-site.xml`[（yarn-default.xml）](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-common/yarn-default.xml) 后分发，停启 yarn。

### 生成 1 个 G 的文本文件

三个小写字母 + 一个 LF

注意要分行……之前就一行，用空格分隔的，读文件的时候堆溢出了，搁那改配置文件去了。

```java
public class Main {
    public static void main(String[] args) throws IOException {
        String filePath = "D:/1/1.txt";

        FileOutputStream fos = new FileOutputStream(filePath);
        BufferedOutputStream bos = new BufferedOutputStream(fos);
        int minAscii = 'a';
        int maxAscii = 'z';
        Random random = new Random();

        long s = 1 << 28;
        byte[] buffer = new byte[4096];

        for (int i = 0; i < s; i++) {
            for (int j = 0; j < 3; j++) {
                byte randomByte = (byte) (minAscii + random.nextInt(maxAscii - minAscii + 1));
                buffer[j] = randomByte;
            }
            buffer[3] = '\n';
            bos.write(buffer, 0, 4);
        }

        bos.flush();
        bos.close();
    }
}
```

选择要提交到的队列

默认是 default

shell：

```sh
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount -D mapreduce.job.queuename=队列名 /wcinput /wcoutput
```

或者 Driver 类里 `conf.set()`

模拟资源紧张环境

```sh
nohup hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar pi 5 2000000 &
```

```sh
nohup hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar pi -D mapreduce.job.priority=5 5 2000000 &
```

执行过程中修改优先级

```sh
yarn application -appID <ApplicationID> -updatePriority 优先级
```

## 驾驶

关键点：

- 车钥匙：点火，使发动机工作
- 离合器踏板（左）：使发动机和变速箱连接到一起。踩下去是离，抬起来是合。挂挡前必踩离合
- 变速杆（档）：改变从变速箱向车轮的转速比例
- 刹车踏板（中）：用于减慢车轮转速
- 手刹：抬起来是刹，把后轮锁死
- 油门踏板（右）：改变流向发动机的汽油流量
- 速度计，右边的是发动机，左边的是车轮

点火前手刹应处于抬起状态，档位为空

平地起步：

1. 点火、踩住离合挂一挡、松手刹
2. 慢松离合

斜坡起步：

1. 点火、踩住离合和刹车挂一挡、松手刹
2. 慢松刹车和离合

停车：

1. 踩住离合和刹车、挂空挡、拉手刹
2. 松脚熄火
